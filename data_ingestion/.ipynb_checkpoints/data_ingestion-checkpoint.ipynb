{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create a data ingenstion pipeline to add data to a vector database. We are going to use `Pinecone` as the vector database, but there are other vector databases available too for example `Chroma, Weaviate, Faiss, etc.`\n",
    "\n",
    "We will be doing the following in this session:\n",
    "- How to load in documents.\n",
    "- Add metadata to each document.\n",
    "- How to use a text splitter to split documents.\n",
    "- How to generate embeddings for each text chunk.\n",
    "- How to insert into a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../images/data_ingestion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You will need a [Pinecone](https://www.pinecone.io/) API key, you can [sign-up](https://app.pinecone.io/?sessionType=signup) for free to get a started account and then get the API key after sign-up.\n",
    "\n",
    "- You will need an [OpenAI](https://openai.com/) api key for this session. It will be provided by Analytics Vidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'os' module for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# Import the 'time' module for handling time-related tasks\n",
    "import time  \n",
    "\n",
    "# Import the 'Pinecone' class from the 'pinecone' package for vector database operations\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Import the 'ServerlessSpec' class from the 'pinecone' package for serverless deployment specifications\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# To create embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# To connect with the Vectorstore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# To parse the PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "\n",
    "# To load files in a directory\n",
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "\n",
    "# To split the text into smaller chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OPENAI_API_KEY environment variable to your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR API KEY HERE>\"\n",
    "\n",
    "# Set the PINECONE_API_KEY environment variable to your Pinecone API key\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"<YOUR API KEY HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a ServerlessSpec object for AWS with the specified region\n",
    "spec = ServerlessSpec(cloud='aws', \n",
    "                      region='us-east-1')\n",
    "\n",
    "# configure client  \n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])  \n",
    "\n",
    "INDEX_NAME = '<YOUR CODE HERE>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the index already exists in the current PC (presumably a database or similar)\n",
    "if INDEX_NAME in pc.list_indexes().names():\n",
    "    # If the index exists, print a message indicating its existence\n",
    "    print(f\"Index `{INDEX_NAME}` already exists\")\n",
    "    \n",
    "    # Retrieve the existing index object\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    # Print detailed statistics about the existing index\n",
    "    print(index.describe_index_stats())\n",
    "    \n",
    "# If the index does not exist, proceed to create a new one\n",
    "else:\n",
    "    # Create a new index with specific parameters\n",
    "    pc.create_index(\n",
    "            INDEX_NAME,\n",
    "            dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "            metric='cosine',\n",
    "            spec=spec\n",
    "        )\n",
    "    \n",
    "    # Wait for the index to be initialized before proceeding\n",
    "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "        # Sleep for 1 second to avoid overloading the system with requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Once the index is ready, print a confirmation message\n",
    "    print(f\"Index with name `{INDEX_NAME}` is created\")\n",
    "    \n",
    "    # Retrieve the newly created index object\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    # Print detailed statistics about the newly created index\n",
    "    print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` In case you want to delete an already existing index then use the following `pc.delete_index(index_name)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path where the data files are stored\n",
    "DATA_DIR_PATH = \"../data/\"\n",
    "\n",
    "# Set the chunk size for processing data, typically in bytes\n",
    "CHUNK_SIZE = 1024\n",
    "\n",
    "# Define the overlap between chunks for more efficient processing\n",
    "CHUNK_OVERLAP = 204\n",
    "\n",
    "# Specify the name of the index to be used for storing or retrieving data\n",
    "INDEX_NAME = 'av-earnings-call'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` Make sure to maintain the below show directory structure since we will be using the Year and Quarter directory names in the metadata later.\n",
    "\n",
    "![Alt text](../images/data_dir_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files\n",
    "\n",
    "Initialize a DirectoryLoader object and pass the `Path to data`, `the type of files to load from directory`, and `the loader_class` which in our case is PyPDFLoader since we are working with PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a loader object with your specific loading logic or method\n",
    "loader = DirectoryLoader(path=DATA_DIR_PATH, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "# Load documents using the loader object\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the total number of documents loaded\n",
    "print(f\"Total documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into the first document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert the Document object to a python dict using the .dict() method.\n",
    "print(f\"keys associated with a Document: {docs[0].dict().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'-'*15}\\nFirst 100 charachters of the page content: {docs[0].page_content[:100]}\\n{'-'*15}\")\n",
    "print(f\"Metadata associated with the document: {docs[0].metadata}\\n{'-'*15}\")\n",
    "print(f\"Datatype of the document: {docs[0].type}\\n{'-'*15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We loop through each document and add additional metadata - filename, quarter, and year\n",
    "for doc in docs:\n",
    "    filename = <YOUR API KEY HERE>\n",
    "    quarter = <YOUR API KEY HERE>\n",
    "    year = <YOUR API KEY HERE>\n",
    "    doc.metadata = {\"filename\": filename, \"quarter\": quarter, \"year\": year, \"source\": doc.dict()['metadata']['source'], \"page\": doc.dict()['metadata']['page']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To veryfy that the metadata is indeed added to the document\n",
    "print(f\"Metadata associated with the document: {docs[0].metadata}\\n{'-'*15}\")\n",
    "print(f\"Metadata associated with the document: {docs[1].metadata}\\n{'-'*15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Text\n",
    "\n",
    "As the name suggests, chunking is the process of dividing a large amount of data into several smaller parts for more effective and meaningful storage.\n",
    "\n",
    "There are various ways to perform chunking naming some as:\n",
    " - Character Chunking\n",
    " - Recursive Character Chunking\n",
    " - Document Specific Chunking\n",
    "\n",
    "For the sake of this session we will be using the `Recursive Character Chunking` and langchain has an implemention that we can directly use. To read more about it you can refer to the [docs](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/)\n",
    "\n",
    "`Additional Resource:` If you want to explore the different chunking stratigies than you can refer to the following docs from langchain - [Link to Docs](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "     chunk_size=CHUNK_SIZE,\n",
    "     chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to confirm if the vectors are already added to the Pinecone database\n",
    "docs_already_in_pinecone = input(\"Are the vectors already added in DB: (Type Y/N)\")\n",
    "\n",
    "# Check if the user has confirmed that the vectors are already in the database\n",
    "if docs_already_in_pinecone == \"Y\" or docs_already_in_pinecone == \"y\":\n",
    "    \n",
    "    # Initialize a PineconeVectorStore object with the existing index and embeddings\n",
    "    docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "    \n",
    "    print(\"Existing Vectorstore is loaded\")\n",
    "    \n",
    "# If the user confirms that the vectors are not in the database, create a new PineconeVectorStore from the documents and embeddings\n",
    "elif docs_already_in_pinecone == \"N\" or docs_already_in_pinecone == \"n\":\n",
    "    \n",
    "    # Create a PineconeVectorStore object from the documents and embeddings, specifying the index name\n",
    "    docsearch = PineconeVectorStore.from_documents(documents, embeddings, index_name=INDEX_NAME)\n",
    "    \n",
    "    print(\"New vectorstore is created and loaded\")\n",
    "    \n",
    "# If the user input is neither 'Y' nor 'N', prompt them to enter a valid response\n",
    "else:\n",
    "    print(\"Please type Y - for yes and N - for no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are defing how to use the loaded vectorstore as retriver\n",
    "retriver = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver.invoke(\"what is the income?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using metadata with retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a retriever object using the `docsearch` module, configured with specific search parameters\n",
    "retriver = docsearch.as_retriever(search_kwargs={\"filter\": {\"quarter\": \"Q1\"}, \"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver.invoke(\"what is the income?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
