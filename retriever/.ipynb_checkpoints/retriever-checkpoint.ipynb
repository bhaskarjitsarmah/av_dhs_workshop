{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn\n",
    "\n",
    "- <b>Setting up the environment</b>:\n",
    "    - Configure environment variables for LangChain, OpenAI, and Pinecone\n",
    "    - Understand the importance of API keys and project settings\n",
    "\n",
    "- <b>Importing necessary libraries</b>:\n",
    "    - Learn about key imports from langchain, including core components and specific modules\n",
    "\n",
    "- <b>Configuring the retrieval setup</b>:\n",
    "    - Set constants for index name, number of top results, and document metadata\n",
    "\n",
    "- <b>Initializing key components</b>:\n",
    "    - Create OpenAI embeddings\n",
    "    - Set up the language model (ChatOpenAI)\n",
    "    - Initialize the vector store (PineconeVectorStore)\n",
    "\n",
    "- <b>Creating a retriever</b>:\n",
    "    - Convert the vector store into a retriever with specific search parameters\n",
    "\n",
    "- <b>Designing the chat template</b>:\n",
    "    - Understand the structure of ChatPromptTemplate\n",
    "    - Learn how to create system and human messages for the chatbot\n",
    "\n",
    "- <b>Implementing helper functions</b>:\n",
    "    - Create a function to format retrieved documents\n",
    "\n",
    "- <b>Building the RAG (Retrieval-Augmented Generation) chain</b>:\n",
    "    - Understand the concept of RunnablePassthrough and RunnableParallel\n",
    "    - Create a chain for processing formatted documents\n",
    "    - Build a chain that combines retrieval and answer generation\n",
    "\n",
    "- <b>Using LangChain Hub</b>:\n",
    "    - Learn how to pull prompts from LangChain Hub\n",
    "    - Understand how to integrate custom prompts into the RAG chain\n",
    "\n",
    "- <b>Pushing prompts to LangChain Hub</b>:\n",
    "    - Learn the process of pushing custom prompts to the hub for reuse\n",
    "\n",
    "- <b>Practical application</b>:\n",
    "    - Run example queries through the RAG chain\n",
    "    - Analyze the output and understand how context retrieval enhances responses\n",
    "\n",
    "- <b>Best practices and considerations</b>:\n",
    "    - Discuss the importance of filtering and metadata in retrieval\n",
    "    - Explore ways to optimize retrieval and response generation\n",
    "\n",
    "- <b>Troubleshooting and debugging</b>:\n",
    "    - Identify common issues in the retrieval pipeline\n",
    "    - Learn techniques for debugging and improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../images/retriever.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'os' module for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# Set the LANGCHAIN_TRACING_V2 environment variable to enable tracing for LangChain version 2\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Set the LANGCHAIN_API_KEY environment variable with your LangChain API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR API KEY HERE>\"\n",
    "\n",
    "# Set the OPENAI_API_KEY environment variable with your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR API KEY HERE>\"\n",
    "\n",
    "# Set the PINECONE_API_KEY environment variable with your Pinecone API key\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"<YOUR API KEY HERE>\"\n",
    "\n",
    "# Set the LANGCHAIN_PROJECT environment variable to specify the project name for LangChain\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"<YOUR PROJECT NAME HERE>\"\n",
    "\n",
    "# Set the LANGSMITH_USER_HANDLE environment variable with your Langsmith user handle\n",
    "os.environ['LANGSMITH_USER_HANDLE'] = \"<YOUR USER HANDLE HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json module for handling JSON data\n",
    "import json\n",
    "\n",
    "# Import ChatOpenAI from langchain_openai for interacting with the OpenAI chat model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import OpenAIEmbeddings from langchain_openai for working with OpenAI embeddings\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "# Import RunnableParallel from langchain_core.runnables for running tasks in parallel\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Import ChatPromptTemplate from langchain_core.prompts for creating chat prompt templates\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import RunnablePassthrough from langchain_core.runnables for running tasks in a passthrough manner\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Import StrOutputParser from langchain_core.output_parsers for parsing string outputs\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import PineconeVectorStore from langchain_pinecone for managing vector storage in Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index name for storing or retrieving data, in this case, 'earning-calls'\n",
    "INDEX_NAME = '<YOUR CODE HERE>'\n",
    "\n",
    "# Set the number of top results to return, here set to 6\n",
    "TOP_K = 6\n",
    "\n",
    "# Specify the quarter for which the data is relevant, in this example, Q1 (Quarter 1)\n",
    "QUARTER = \"Q1\"\n",
    "\n",
    "# Define the filename of the document being processed, in this case, \"Adani Enterprises Ltd.pdf\"\n",
    "FILENAME = \"Adani Enterprises Ltd.pdf\"\n",
    "\n",
    "# Specify the fiscal year for the data, in this example, FY24 (Fiscal Year 2024)\n",
    "YEAR = \"FY24\"\n",
    "\n",
    "# Initialize OpenAIEmbeddings with the specified model for generating text embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Initialize ChatOpenAI with the specified model and temperature for generating chat responses\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PineconeVectorStore instance with the specified index name and embedding model\n",
    "index = PineconeVectorStore(index_name=INDEX_NAME, \n",
    "                            embedding=embeddings) \n",
    "\n",
    "# Convert the PineconeVectorStore instance into a retriever object for searching\n",
    "retriver = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver.invoke(\"what is the capex?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This template is designed for a chatbot that acts as an expert Q&A system\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # Define the system message that outlines the chatbot's role and rules\n",
    "        (\n",
    "        \"system\", \"\"\"You are an expert Q&A system that is trusted around the world. Always answer the query using the \n",
    "        provided context information, and not prior knowledge. Some rules to follow:\n",
    "        \n",
    "        1. Never directly reference the given context in your answer.\n",
    "        2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"\"\"\n",
    "        ),\n",
    "        \n",
    "        # Define the human message that provides context information and a query\n",
    "        (\n",
    "        \"human\", \"\"\"Context information is below.\n",
    "        \\n---------------------\\n\n",
    "        {context}\n",
    "        \\n---------------------\\n\n",
    "        \n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer: \"\"\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_template.format(context=\"This is a sample context to see how the prompt looks like\", \n",
    "                           query=\"This is a sample query?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to format documents by joining their page content with newlines\n",
    "def format_docs(docs):\n",
    "    # Join the page content of each document with two newlines between them\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create a runnable chain for generating responses from formatted documents\n",
    "# This chain starts with formatting the documents, then uses the chat template,\n",
    "# processes the response through the language model, and finally parses the output\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | chat_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a runnable chain for retrieving context and generating answers in parallel\n",
    "# This chain retrieves context using the retriever and formats the query,\n",
    "# then generates answers using the previously defined chain for formatted documents\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriver, \"query\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain_with_source.invoke(\"What was the income?\")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Versioning\n",
    "\n",
    "Loading a Specific Version of a Prompt:\n",
    "\n",
    "1. **Version Tracking in Repositories:**\n",
    "   - Each push to a prompt repository saves a new version, identified by a unique commit hash.\n",
    "\n",
    "2. **Loading the Latest Version:**\n",
    "   - By default, accessing the repo will load the most recent version of a given prompt.\n",
    "\n",
    "3. **Loading a Specific Version:**\n",
    "   - To load a specific version, include its commit hash with the prompt name.\n",
    "   - Example: For loading the \"earnings-call-rag\" with version `6214c98a`, append this hash to the prompt name in your loading command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"bhaskarjit/earnings-call-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.format(context=\"This is a sample context to see how the prompt looks like\", \n",
    "                    query=\"This is a sample query?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = <YOUR CODE HERE>\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriver, \"query\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain_with_source.invoke(\"What was the income?\")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Share Prompts on LangChain Hub:\n",
    "\n",
    "1. **Getting Started:**\n",
    "   - Getting prompts from LangChain Hub is easy, and so is sharing your own prompts.\n",
    "\n",
    "   - This lets you easily share and manage your own prompts.\n",
    "\n",
    "2. **Making a Prompt:**\n",
    "\n",
    "   - First, create a prompt that fits what you need.\n",
    "\n",
    "   - Make sure it follows the rules of LangChain Hub.\n",
    "\n",
    "3. **Sharing Process:**\n",
    "\n",
    "   - The sharing has two important parts:\n",
    "     - **Account Handle:** Your special name in LangChain Hub, like `me-langchain-user`.\n",
    "     - **Prompt Name:** A clear name for your prompt, showing what it does.\n",
    "\n",
    "4. **How to Share with Code:**\n",
    "   - This is a simple way to share:\n",
    "     ```python\n",
    "     from langchain import hub\n",
    "\n",
    "     # Define your prompt\n",
    "     my_prompt = \"...\"  # Your prompt content goes here\n",
    "\n",
    "     # Share it on LangChain Hub\n",
    "     hub.push(f\"{account_handle}/{prompt_name}\", my_prompt)\n",
    "     ```\n",
    "     - Replace `account_handle` with your username and `my_prompt` with your prompt's name.\n",
    "     \n",
    "     - Make sure `my_prompt` follows LangChain PromptTemplate.\n",
    "\n",
    "5. **Using Your Shared Prompt:**\n",
    "   - Once it's shared, you can use it in different apps through LangChain Hub.\n",
    "   - This makes it easy to share with others, work together, and manage your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt.messages), prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[1].prompt.template = \"\"\"Context information is below.\n",
    "\\n---------------------\\n\n",
    "{context}\n",
    "\\n---------------------\\n\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Use Bullet poits whenever possible in the answer.\n",
    "Query: {query}\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added the following line in the prompt: `Use Bullet poits whenever possible in the answer.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"bhaskarjit\"\n",
    "prompt_url = hub.push(f\"{handle}/av-earnings-call-rag\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
